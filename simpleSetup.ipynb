{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as p\n",
    "import email as em\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages(filename):\n",
    "    email_list = []\n",
    "    parsed = []\n",
    "    cur_par = []\n",
    "    with open(filename) as f:\n",
    "        #split each email based on 'Date' line\n",
    "        for line in f:\n",
    "            if ('Date' in line.strip()) and cur_par:\n",
    "                parsed.append(cur_par)\n",
    "                cur_par = []\n",
    "            else:\n",
    "                cur_par.append(line.strip())\n",
    "    #parse into email type object\n",
    "    for email in parsed:\n",
    "        #join the email into a single string\n",
    "        email_string = \"\\n\".join(email)\n",
    "        #read the string\n",
    "        email_object = em.message_from_string(email_string)\n",
    "        #add email type into list\n",
    "        email_list.append(email_object)\n",
    "    \n",
    "    return email_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = get_messages('help2002-2017.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_data[3]._payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [em['Subject'].lower()  for em in file_data[1:]]\n",
    "data = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenization of each document\n",
    "tokenized_doc = []\n",
    "for d in data:\n",
    "    tokenized_doc.append(word_tokenize(d.lower()))\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]\n",
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size=512, window=10, min_count=1, workers=8, epochs = 512)\n",
    "# Save trained doc2vec model\n",
    "model.save(\"test_doc2vec.model\")\n",
    "## Load saved doc2vec model\n",
    "model= Doc2Vec.load(\"test_doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most similar doc \n",
    "test_doc = word_tokenize(\"lab 3 questions\".lower())\n",
    "model.docvecs.most_similar(positive=[model.infer_vector(test_doc)],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[33]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
